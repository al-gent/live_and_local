{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from datetime import datetime\n",
    "import psycopg2\n",
    "from psycopg2.extras import RealDictCursor\n",
    "import os\n",
    "\n",
    "def start_selenium():\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.page_load_strategy = 'eager'\n",
    "    chrome_options.add_argument(\"--disable-images\")\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    driver.set_page_load_timeout(12)\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_venue(venue_id, venue_name, url, scraping_config, driver):\n",
    "    \"\"\"\n",
    "    Generic scraper that works for any venue based on its config\n",
    "    Returns list of raw events: [(artist, date_text, genre, is_cancelled), ...]\n",
    "    \"\"\"\n",
    "    print(f\"🔍 Scraping {venue_name}...\")\n",
    "    \n",
    "    soups = []\n",
    "    pagination = scraping_config.get('pagination', {})\n",
    "    \n",
    "    try:\n",
    "        # Handle pagination\n",
    "        if pagination.get('enabled'):\n",
    "            pages = pagination.get('pages', 1)\n",
    "            url_pattern = pagination.get('url_pattern')\n",
    "            \n",
    "            for i in range(1, pages + 1):\n",
    "                page_url = url_pattern.format(page=i)\n",
    "                print(f\"  → Getting page {i}: {page_url}\")\n",
    "                try:\n",
    "                    driver.get(page_url)\n",
    "                    time.sleep(3)\n",
    "                    soups.append(BeautifulSoup(driver.page_source, 'html.parser'))\n",
    "                except Exception as e:\n",
    "                    print(f\"  ⚠️  Timeout/error loading page {i}: {e}\")\n",
    "                    continue\n",
    "        else:\n",
    "            # No pagination, just get the base URL\n",
    "            print(f\"  → Getting {url}\")\n",
    "            driver.get(url)\n",
    "            time.sleep(3)\n",
    "            soups.append(BeautifulSoup(driver.page_source, 'html.parser'))\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ Failed to scrape {venue_name}: {e}\")\n",
    "        return []\n",
    "    \n",
    "    # Extract events from all pages\n",
    "    events = []\n",
    "    selectors = scraping_config['selectors']\n",
    "    \n",
    "    for soup in soups:\n",
    "        # Find all event containers\n",
    "        event_containers = soup.select(selectors['event_container'])\n",
    "        print(f\"  → Found {len(event_containers)} events on this page\")\n",
    "        \n",
    "        for container in event_containers:\n",
    "            try:\n",
    "                # Extract artist\n",
    "                artist_elem = container.select_one(selectors['artist'])\n",
    "                artist = artist_elem.text.strip() if artist_elem else None\n",
    "                \n",
    "                # Extract date\n",
    "                date_elem = container.select_one(selectors['date'])\n",
    "                date_text = date_elem.text.strip() if date_elem else None\n",
    "                \n",
    "                # Extract genre (if configured)\n",
    "                genre = None\n",
    "                if selectors.get('genre'):\n",
    "                    genre_elem = container.select_one(selectors['genre'])\n",
    "                    genre = genre_elem.text.strip() if genre_elem else None\n",
    "                \n",
    "                # Check if cancelled (if configured)\n",
    "                is_cancelled = False\n",
    "                if selectors.get('cancellation_indicator'):\n",
    "                    cancel_elem = container.select_one(selectors['cancellation_indicator'])\n",
    "                    if cancel_elem:\n",
    "                        cancelled_text = scraping_config.get('filters', {}).get('cancelled_text', 'Cancelled')\n",
    "                        is_cancelled = cancel_elem.text.strip() == cancelled_text\n",
    "                \n",
    "                # Only add if we got at minimum an artist and date\n",
    "                if artist and date_text:\n",
    "                    events.append({\n",
    "                        'venue_id': venue_id,\n",
    "                        'artist': artist,\n",
    "                        'date_text': date_text,\n",
    "                        'genre': genre,\n",
    "                        'is_cancelled': is_cancelled\n",
    "                    })\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  ⚠️  Error parsing event: {e}\")\n",
    "                continue\n",
    "    \n",
    "    print(f\"✅ Scraped {len(events)} events from {venue_name}\")\n",
    "    return events\n",
    "\n",
    "def scrape_all_venues(conn):\n",
    "    \"\"\"\n",
    "    Scrape all active venues from the database\n",
    "    Returns list of all events from all venues\n",
    "    \"\"\"\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    # Get all active venues\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT venue_id, name, url, scraping_config \n",
    "        FROM venues \n",
    "        WHERE is_active = TRUE\n",
    "        ORDER BY name;\n",
    "    \"\"\")\n",
    "    \n",
    "    venues = cur.fetchall()\n",
    "    print(f\"📍 Found {len(venues)} active venues to scrape\\n\")\n",
    "    \n",
    "    # Start selenium once for all venues\n",
    "    driver = start_selenium()\n",
    "    \n",
    "    all_events = []\n",
    "    \n",
    "    try:\n",
    "        for venue_id, name, url, config in venues:\n",
    "            events = scrape_venue(venue_id, name, url, config, driver)\n",
    "            all_events.extend(events)\n",
    "            print()  # Blank line between venues\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error scraping {name}: {e}\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "        print(\"🛑 Browser closed\")\n",
    "    \n",
    "    print(f\"\\n🎉 Total events scraped: {len(all_events)}\")\n",
    "    return all_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "conn = psycopg2.connect(os.getenv('DATABASE_URL_UNPOOLED'))\n",
    "all_events = scrape_all_venues(conn)\n",
    "events = pd.DataFrame(all_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'apply'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 28\u001b[0m\n\u001b[1;32m     22\u001b[0m cur\u001b[38;5;241m.\u001b[39mexecute(\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124m    SELECT venue_id, scraping_config->>\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate_format\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m as date_format\u001b[39m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124m    FROM venues \u001b[39m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m venue_id_date_format \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(cur\u001b[38;5;241m.\u001b[39mfetchall())\n\u001b[0;32m---> 28\u001b[0m all_events[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevent_date\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mall_events\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m(\u001b[38;5;28;01mlambda\u001b[39;00m row: parse_date(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraw_date_text\u001b[39m\u001b[38;5;124m'\u001b[39m], venue_id_date_format[row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvenue_id\u001b[39m\u001b[38;5;124m'\u001b[39m]]), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     30\u001b[0m all_events\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'apply'"
     ]
    }
   ],
   "source": [
    "from functions import parse_date\n",
    "\n",
    "conn = psycopg2.connect(os.getenv('DATABASE_URL_UNPOOLED'))\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"\"\"\n",
    "    SELECT venue_id, scraping_config->>'date_format' as date_format\n",
    "    FROM venues \n",
    "\"\"\")\n",
    "venue_id_date_format = dict(cur.fetchall())\n",
    "\n",
    "all_events['event_date'] = all_events.apply(lambda row: parse_date(row['raw_date_text'], venue_id_date_format[row['venue_id']]), axis=1)\n",
    "\n",
    "all_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Events saved to events_data.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(all_events)\n",
    "\n",
    "# Save as CSV\n",
    "df.to_csv('events_data.csv', index=False)\n",
    "\n",
    "print(\"Events saved to events_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
