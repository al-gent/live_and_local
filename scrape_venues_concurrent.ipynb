{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concurrent Venue Scraping Experiment\n",
    "\n",
    "This notebook tests a more scalable approach to scraping hundreds of venues concurrently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from datetime import datetime, date\n",
    "import psycopg2\n",
    "from psycopg2.extras import RealDictCursor\n",
    "import os\n",
    "import json\n",
    "from populate_events_functions import start_selenium, parse_date, scrape_venue_html, scrape_venue_json_ld\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Venues from Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 13 active venues:\n",
      "  - Ace of Spades\n",
      "  - Avogardro's Number\n",
      "  - Goldfield Trading Post\n",
      "  - Neck of the Woods\n",
      "  - Rickshaw Stop\n",
      "  - The Aggie Theatre\n",
      "  - The Armory\n",
      "  - The Chapel\n",
      "  - The Great American Music Hall\n",
      "  - The Independent\n",
      "  - The Mishawaka \n",
      "  - The Warfield\n",
      "  - Washington's\n"
     ]
    }
   ],
   "source": [
    "conn = psycopg2.connect(os.getenv('DATABASE_URL_UNPOOLED'))\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Get all active venues\n",
    "cur.execute(\"\"\"\n",
    "    SELECT * \n",
    "    FROM venues \n",
    "    WHERE is_active = TRUE\n",
    "    ORDER BY name;\n",
    "\"\"\")\n",
    "\n",
    "column_names = [desc[0] for desc in cur.description]\n",
    "res = cur.fetchall()\n",
    "venues = [dict(zip(column_names, v)) for v in res]\n",
    "\n",
    "cur.close()\n",
    "conn.close()\n",
    "\n",
    "print(f\"Loaded {len(venues)} active venues:\")\n",
    "for venue in venues:\n",
    "    print(f\"  - {venue['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Scraping Functions\n",
    "\n",
    "Clean functions with driver reuse for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_single_venue_with_driver(venue_dict, driver):\n",
    "    \"\"\"\n",
    "    Scrapes a single venue using a provided driver (for reuse).\n",
    "    Handles pagination and errors gracefully.\n",
    "    \n",
    "    Returns: (venue_name, events_list, error_message)\n",
    "    \"\"\"\n",
    "    venue_name = venue_dict['name']\n",
    "    venue_id = venue_dict['venue_id']\n",
    "    scraping_config = venue_dict.get('scraping_config', {})\n",
    "    \n",
    "    events = []\n",
    "    error = None\n",
    "    \n",
    "    try:\n",
    "        # Extract config\n",
    "        pagination = scraping_config.get('pagination', {})\n",
    "        base_url = scraping_config.get('base_url')\n",
    "        method = scraping_config.get('scraping_method', 'html')\n",
    "        \n",
    "        # Determine if pagination is enabled\n",
    "        if pagination.get('enabled'):\n",
    "            # Paginated scraping\n",
    "            url_pattern = pagination.get('url_pattern')\n",
    "            max_pages = pagination.get('pages', 5)\n",
    "            \n",
    "            for page_num in range(1, max_pages + 1):\n",
    "                page_url = url_pattern.format(page=page_num)\n",
    "                \n",
    "                try:\n",
    "                    driver.get(page_url)\n",
    "                    time.sleep(1)\n",
    "                    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "                    \n",
    "                    # Scrape using configured method\n",
    "                    page_events = scrape_venue_html(soup, venue_id, scraping_config)\n",
    "                    events.extend(page_events)\n",
    "                    \n",
    "                    # Stop if we got no events (likely end of pagination)\n",
    "                    if not page_events:\n",
    "                        break\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    # Log page error but continue to next page\n",
    "                    print(f\"  ⚠️  {venue_name} - Error on page {page_num}: {e}\")\n",
    "                    break\n",
    "        else:\n",
    "            # Single page scraping\n",
    "            driver.get(base_url)\n",
    "            time.sleep(1)\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            \n",
    "            if method == 'html':\n",
    "                events = scrape_venue_html(soup, venue_id, scraping_config)\n",
    "            elif method == 'json-ld':\n",
    "                events = scrape_venue_json_ld(soup, venue_id, scraping_config)\n",
    "    \n",
    "    except Exception as e:\n",
    "        error = str(e)\n",
    "        print(f\"  ❌ {venue_name} - Fatal error: {e}\")\n",
    "    \n",
    "    return venue_name, events, error\n",
    "\n",
    "\n",
    "def scrape_venues_worker(venues_chunk):\n",
    "    \"\"\"\n",
    "    Worker function that creates ONE driver and reuses it for all venues in the chunk.\n",
    "    This is much more efficient than creating a driver per venue.\n",
    "    \n",
    "    Returns: list of (venue_name, events_list, error_message) tuples\n",
    "    \"\"\"\n",
    "    driver = None\n",
    "    results = []\n",
    "    \n",
    "    try:\n",
    "        # Create one driver for this worker thread\n",
    "        driver = start_selenium()\n",
    "        \n",
    "        # Process all venues assigned to this worker\n",
    "        for venue in venues_chunk:\n",
    "            result = scrape_single_venue_with_driver(venue, driver)\n",
    "            results.append(result)\n",
    "    \n",
    "    finally:\n",
    "        # Clean up the driver when done with all venues\n",
    "        if driver:\n",
    "            driver.quit()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Scrape All Venues Concurrently\n",
    "\n",
    "Now let's run multiple venues in parallel for speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 13 venues with 3 workers\n",
      "Chunk sizes: [5, 4, 4]\n",
      "\n",
      "  ⚠️ Error parsing event: time data 'Sat, June 20, 2026' does not match format '%a, %b %d, %Y'\n",
      "[1/13] ✅ Ace of Spades - 0 events\n",
      "[2/13] ✅ Avogardro's Number - 23 events\n",
      "[3/13] ✅ Goldfield Trading Post - 20 events\n",
      "[4/13] ✅ Neck of the Woods - 36 events\n",
      "[5/13] ✅ Rickshaw Stop - 30 events\n",
      "  ⚠️ Error parsing event: time data 'TBD' does not match format '%a, %b %d, %Y'\n",
      "[6/13] ✅ The Aggie Theatre - 43 events\n",
      "[7/13] ✅ The Armory - 9 events\n",
      "[8/13] ✅ The Chapel - 27 events\n",
      "[9/13] ✅ The Great American Music Hall - 12 events\n",
      "[10/13] ✅ The Independent - 73 events\n",
      "[11/13] ✅ The Mishawaka  - 15 events\n",
      "[12/13] ✅ The Warfield - 30 events\n",
      "[13/13] ✅ Washington's - 9 events\n",
      "\n",
      "============================================================\n",
      "SCRAPING COMPLETE\n",
      "============================================================\n",
      "Total events scraped: 327\n",
      "Successful venues: 13\n",
      "Failed venues: 0\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "MAX_WORKERS = 3  # Number of concurrent scrapers (adjust based on your system)\n",
    "\n",
    "# Split venues into chunks for each worker\n",
    "def chunk_list(lst, n):\n",
    "    \"\"\"Split list into n roughly equal chunks\"\"\"\n",
    "    k, m = divmod(len(lst), n)\n",
    "    return [lst[i*k+min(i,m):(i+1)*k+min(i+1,m)] for i in range(n)]\n",
    "\n",
    "venue_chunks = chunk_list(venues, MAX_WORKERS)\n",
    "\n",
    "print(f\"Processing {len(venues)} venues with {MAX_WORKERS} workers\")\n",
    "print(f\"Chunk sizes: {[len(chunk) for chunk in venue_chunks]}\\n\")\n",
    "\n",
    "all_events = []\n",
    "failed_venues = []\n",
    "completed = 0\n",
    "\n",
    "# Use ThreadPoolExecutor with worker function that reuses drivers\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    # Submit chunks to workers (each worker gets its own driver)\n",
    "    futures = [executor.submit(scrape_venues_worker, chunk) for chunk in venue_chunks]\n",
    "    \n",
    "    # Process results as they complete\n",
    "    for future in as_completed(futures):\n",
    "        try:\n",
    "            # Each future returns a list of results from one worker\n",
    "            worker_results = future.result()\n",
    "            \n",
    "            for venue_name, events, error in worker_results:\n",
    "                completed += 1\n",
    "                \n",
    "                if error:\n",
    "                    print(f\"[{completed}/{len(venues)}] ❌ {venue_name} - Error: {error}\")\n",
    "                    failed_venues.append({'name': venue_name, 'error': error})\n",
    "                else:\n",
    "                    print(f\"[{completed}/{len(venues)}] ✅ {venue_name} - {len(events)} events\")\n",
    "                    all_events.extend(events)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Worker thread failed: {e}\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"SCRAPING COMPLETE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total events scraped: {len(all_events)}\")\n",
    "print(f\"Successful venues: {len(venues) - len(failed_venues)}\")\n",
    "print(f\"Failed venues: {len(failed_venues)}\")\n",
    "\n",
    "if failed_venues:\n",
    "    print(f\"\\nFailed venues:\")\n",
    "    for fail in failed_venues:\n",
    "        print(f\"  - {fail['name']}: {fail['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Convert to DataFrame and Remove Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique events: 298\n",
      "\n",
      "Sample of scraped data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>venue_id</th>\n",
       "      <th>raw_event_name</th>\n",
       "      <th>raw_date_text</th>\n",
       "      <th>genres</th>\n",
       "      <th>is_cancelled</th>\n",
       "      <th>parsed_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32</td>\n",
       "      <td>Al Loves El</td>\n",
       "      <td>Thursday October 23rd</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>2025-10-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32</td>\n",
       "      <td>Amanda Hofer Trio</td>\n",
       "      <td>Thursday October 23rd</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>2025-10-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32</td>\n",
       "      <td>Defunkl</td>\n",
       "      <td>Friday October 24th</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>2025-10-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32</td>\n",
       "      <td>High Desert Groove</td>\n",
       "      <td>Friday October 24th</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>2025-10-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>Dracula's Disco</td>\n",
       "      <td>Saturday October 25th</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>2025-10-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>32</td>\n",
       "      <td>Harmony Hotshots</td>\n",
       "      <td>Sunday October 26th</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>2025-10-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>32</td>\n",
       "      <td>School of Rock Adult Band</td>\n",
       "      <td>Sunday October 26th</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>2025-10-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>32</td>\n",
       "      <td>Blue Monday</td>\n",
       "      <td>Monday October 27th</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>2025-10-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>32</td>\n",
       "      <td>Open Mic!</td>\n",
       "      <td>Tuesday October 28th</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>2025-10-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>32</td>\n",
       "      <td>An Evening with That 1 Guy</td>\n",
       "      <td>Thursday October 30th</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>2025-10-30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   venue_id              raw_event_name          raw_date_text genres  \\\n",
       "0        32                 Al Loves El  Thursday October 23rd   None   \n",
       "1        32           Amanda Hofer Trio  Thursday October 23rd   None   \n",
       "2        32                     Defunkl    Friday October 24th   None   \n",
       "3        32          High Desert Groove    Friday October 24th   None   \n",
       "4        32             Dracula's Disco  Saturday October 25th   None   \n",
       "5        32            Harmony Hotshots    Sunday October 26th   None   \n",
       "6        32   School of Rock Adult Band    Sunday October 26th   None   \n",
       "7        32                 Blue Monday    Monday October 27th   None   \n",
       "8        32                   Open Mic!   Tuesday October 28th   None   \n",
       "9        32  An Evening with That 1 Guy  Thursday October 30th   None   \n",
       "\n",
       "   is_cancelled parsed_date  \n",
       "0         False  2025-10-23  \n",
       "1         False  2025-10-23  \n",
       "2         False  2025-10-24  \n",
       "3         False  2025-10-24  \n",
       "4         False  2025-10-25  \n",
       "5         False  2025-10-26  \n",
       "6         False  2025-10-26  \n",
       "7         False  2025-10-27  \n",
       "8         False  2025-10-28  \n",
       "9         False  2025-10-30  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df = pd.DataFrame(all_events).drop_duplicates()\n",
    "\n",
    "print(f\"Total unique events: {len(raw_df)}\")\n",
    "print(f\"\\nSample of scraped data:\")\n",
    "raw_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Explore the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Events per venue:\n",
      "  The Independent: 72 events\n",
      "  The Aggie Theatre: 43 events\n",
      "  The Warfield: 30 events\n",
      "  Rickshaw Stop: 25 events\n",
      "  Avogardro's Number: 23 events\n",
      "  The Chapel: 23 events\n",
      "  Goldfield Trading Post: 20 events\n",
      "  Neck of the Woods: 18 events\n",
      "  The Mishawaka : 14 events\n",
      "  The Great American Music Hall: 12 events\n",
      "  The Armory: 9 events\n",
      "  Washington's: 9 events\n"
     ]
    }
   ],
   "source": [
    "# Events per venue\n",
    "print(\"Events per venue:\")\n",
    "if 'venue_id' in raw_df.columns and len(raw_df) > 0:\n",
    "    venue_counts = raw_df['venue_id'].value_counts()\n",
    "    for venue_id, count in venue_counts.items():\n",
    "        venue_name = next(v['name'] for v in venues if v['venue_id'] == venue_id)\n",
    "        print(f\"  {venue_name}: {count} events\")\n",
    "else:\n",
    "    print(\"  No data to display\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. Test this approach with your current venues\n",
    "2. Adjust `MAX_WORKERS` based on performance (3-5 is usually good)\n",
    "3. If it works well, we can integrate this into `populate_events.ipynb`\n",
    "4. Add retry logic for failed venues if needed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
