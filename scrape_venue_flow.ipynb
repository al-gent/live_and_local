{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from datetime import datetime\n",
    "import psycopg2\n",
    "from psycopg2.extras import RealDictCursor\n",
    "import os\n",
    "import json\n",
    "from functions import start_selenium, parse_date\n",
    "import html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have been having a hard time wrapping my head around how to efficiently organize scraping.\n",
    "\n",
    "Chat wrote a good function based on the functions that I wrote, but it is confusing and i'd like to understand it better, so I'm goign to start by going through those functions and trying to rewrite them in a way that makes sense to me.\n",
    "\n",
    "The goal here is to have a script that will pull from VENUES and scrape and then validate each of the artists performing at the venues, and then populate the ARTIST_EVENTS table.\n",
    "\n",
    "Ok so thats 3 different tasks, lets break them down.\n",
    "\n",
    "1. scrape the venues \n",
    "\n",
    "We have a few choices to make with the architeture. Ideally, we would be able to use something just like a scraping_params dict  \n",
    "       \"scraping_config\": {\n",
    "            \"base_url\": \"https://rickshawstop.com/?list1page=1\",\n",
    "            \"pagination\": {\n",
    "                \"enabled\": True,\n",
    "                \"pages\": 2,\n",
    "                \"url_pattern\": \"https://rickshawstop.com/?list1page={page}\"\n",
    "            },\n",
    "            \"selectors\": {\n",
    "                \"event_container\": \"div.event-info-block\",\n",
    "                \"artist\": \"p.fs-12.headliners\",\n",
    "                \"date\": \"p.fs-18.bold.mt-1r.date\",\n",
    "                \"genre\": None,\n",
    "                \"cancellation_indicator\": None\n",
    "            },\n",
    "            \"date_format\": \"%a %b %d\",\n",
    "            \"filters\": {\n",
    "                \"exclude_genres\": [],\n",
    "                \"check_cancelled\": False\n",
    "            }}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['venue_id', 'name', 'address', 'city', 'url', 'scraping_config', 'is_active', 'created_at', 'updated_at']\n"
     ]
    }
   ],
   "source": [
    "conn = psycopg2.connect(os.getenv('DATABASE_URL_UNPOOLED'))\n",
    "\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Get all active venues\n",
    "cur.execute(\"\"\"\n",
    "    SELECT * \n",
    "    FROM venues \n",
    "    WHERE is_active = TRUE\n",
    "    ORDER BY name;\n",
    "\"\"\")\n",
    "\n",
    "column_names = [desc[0] for desc in cur.description]\n",
    "print(\"Columns:\", column_names)\n",
    "\n",
    "# Get results\n",
    "res = cur.fetchall()\n",
    "venues = [dict(zip(column_names, v)) for v in res]\n",
    "# ret = dict(zip(column_names, venues[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_venue_html(soup, venue_id, scraping_config):\n",
    "    \"\"\"\n",
    "    Scraper for venues using HTML/CSS selectors\n",
    "    Returns list of events\n",
    "    \"\"\"\n",
    "    selectors = scraping_config['selectors']\n",
    "    date_format = scraping_config['date_format']\n",
    "    event_containers = soup.select(selectors['event_container'])\n",
    "    \n",
    "    events = []\n",
    "    \n",
    "    for container in event_containers:\n",
    "        try:\n",
    "            # Extract artists\n",
    "            artist_elem = container.select_one(selectors['artist'])\n",
    "            artist = artist_elem.text.strip() if artist_elem else None\n",
    "            \n",
    "            # Extract date\n",
    "            date_elem = container.select_one(selectors['date'])\n",
    "            date_text = date_elem.text.strip() if date_elem else None\n",
    "            parsed_date = parse_date(date_text, date_format) if date_text else None\n",
    "            \n",
    "            # Extract genre (if configured)\n",
    "            genre = None\n",
    "            if selectors.get('genre'):\n",
    "                genre_elem = container.select_one(selectors['genre'])\n",
    "                genre = genre_elem.text.strip() if genre_elem else None\n",
    "            \n",
    "            # Check if cancelled (if configured)\n",
    "            is_cancelled = False\n",
    "            if selectors.get('cancellation_indicator'):\n",
    "                cancel_elem = container.select_one(selectors['cancellation_indicator'])\n",
    "                if cancel_elem:\n",
    "                    cancelled_text = scraping_config.get('filters', {}).get('cancelled_text', 'Cancelled')\n",
    "                    is_cancelled = cancel_elem.text.strip() == cancelled_text\n",
    "            \n",
    "            # Only add if we got at minimum an artist and date\n",
    "            if artist and date_text:\n",
    "                events.append({\n",
    "                    'venue_id': venue_id,\n",
    "                    'raw_event_name': artist,\n",
    "                    'raw_date_text': date_text,\n",
    "                    'genres': genre,\n",
    "                    'is_cancelled': is_cancelled,\n",
    "                    'parsed_date': parsed_date\n",
    "                })\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö†Ô∏è Error parsing event: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_venue_json_ld(soup, venue_id, scraping_config):\n",
    "    \"\"\"\n",
    "    Scraper for venues using JSON-LD structured data\n",
    "    Returns list of events\n",
    "    \"\"\"\n",
    "    json_keys = scraping_config.get('json_keys')\n",
    "    if not json_keys:\n",
    "        print(f\"  ‚ùå Missing json_keys in config\")\n",
    "        return []\n",
    "    \n",
    "    # Find all JSON-LD script tags\n",
    "    script_tags = soup.find_all('script', type='application/ld+json')\n",
    "    events = []\n",
    "    \n",
    "    for script_tag in script_tags:\n",
    "        try:\n",
    "            # Clean control characters before parsing\n",
    "            json_text = script_tag.string\n",
    "            if json_text:\n",
    "                json_text = json_text.replace('\\n', ' ').replace('\\r', ' ').replace('\\t', ' ')\n",
    "                event_data = json.loads(json_text)\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            # Skip if it's not an Event schema\n",
    "            if event_data.get('@type') != 'Event':\n",
    "                continue\n",
    "            \n",
    "            # Extract artist\n",
    "            artist = get_nested_value(event_data, json_keys.get('artist', 'performer'))\n",
    "            artist = html.unescape(str(artist)).strip() if artist else None\n",
    "            \n",
    "            # Extract date\n",
    "            date_string = get_nested_value(event_data, json_keys.get('date', 'startDate'))\n",
    "            \n",
    "            # Parse date based on format\n",
    "            parsed_date = None\n",
    "            date_text = None\n",
    "            if date_string:\n",
    "                try:\n",
    "                    date_format = scraping_config.get('date_format', 'iso')\n",
    "                    if date_format == 'iso':\n",
    "                        # Handle various ISO formats\n",
    "                        parsed_date = datetime.fromisoformat(date_string.replace('+00:00', '').replace('Z', ''))\n",
    "                        date_text = parsed_date.strftime('%Y-%m-%d')\n",
    "                    else:\n",
    "                        parsed_date = datetime.strptime(date_string, date_format)\n",
    "                        date_text = parsed_date.strftime('%Y-%m-%d')\n",
    "                except Exception as e:\n",
    "                    print(f\"  ‚ö†Ô∏è Error parsing date '{date_string}': {e}\")\n",
    "                    date_text = date_string  # Fallback to raw string\n",
    "            \n",
    "            # Only add if we got at minimum an artist and date\n",
    "            if artist and date_text:\n",
    "                events.append({\n",
    "                    'venue_id': venue_id,\n",
    "                    'raw_event_name': artist,\n",
    "                    'raw_date_text': date_text,\n",
    "                    'genres': None,\n",
    "                    'is_cancelled': False,\n",
    "                    'parsed_date': parsed_date\n",
    "                })\n",
    "                \n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"  ‚ö†Ô∏è Skipping malformed JSON-LD script\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö†Ô∏è Error extracting event data: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return events\n",
    "\n",
    "def get_nested_value(data, key_path):\n",
    "    \"\"\"\n",
    "    Get value from nested dict using dot notation\n",
    "    e.g., 'location.name' returns data['location']['name']\n",
    "    \"\"\"\n",
    "    if not key_path:\n",
    "        return None\n",
    "        \n",
    "    keys = key_path.split('.')\n",
    "    value = data\n",
    "    \n",
    "    for key in keys:\n",
    "        if isinstance(value, dict):\n",
    "            value = value.get(key)\n",
    "        else:\n",
    "            return None\n",
    "            \n",
    "        if value is None:\n",
    "            return None\n",
    "    \n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neck of the Woods\n",
      "added  36\n",
      "Rickshaw Stop\n",
      "  ‚Üí Getting page 1: https://rickshawstop.com/?list1page=1\n",
      "added  15\n",
      "  ‚Üí Getting page 2: https://rickshawstop.com/?list1page=2\n",
      "added  15\n",
      "  ‚Üí Getting page 3: https://rickshawstop.com/?list1page=3\n",
      "added  15\n",
      "  ‚Üí Getting page 4: https://rickshawstop.com/?list1page=4\n",
      "added  15\n",
      "  ‚Üí Getting page 5: https://rickshawstop.com/?list1page=5\n",
      "added  10\n",
      "added  10\n",
      "The Aggie Theatre\n",
      "added  45\n",
      "The Chapel\n",
      "  ‚Üí Getting page 1: https://thechapelsf.com/music/?list1page=1\n",
      "added  15\n",
      "  ‚Üí Getting page 2: https://thechapelsf.com/music/?list1page=2\n",
      "added  15\n",
      "  ‚Üí Getting page 3: https://thechapelsf.com/music/?list1page=3\n",
      "added  15\n",
      "  ‚Üí Getting page 4: https://thechapelsf.com/music/?list1page=4\n",
      "added  15\n",
      "  ‚Üí Getting page 5: https://thechapelsf.com/music/?list1page=5\n",
      "added  15\n",
      "added  15\n",
      "The Great American Music Hall\n",
      "added  10\n",
      "The Independent\n",
      "added  81\n",
      "The Mishawaka \n",
      "  ‚ö†Ô∏è Error parsing event: time data 'Sat, June 20, 2026' does not match format '%a, %b %d, %Y'\n",
      "added  16\n",
      "The Warfield\n",
      "  ‚ö†Ô∏è Error parsing event: time data 'TBD' does not match format '%a, %b %d, %Y'\n",
      "added  34\n",
      "Washington's\n",
      "added  9\n"
     ]
    }
   ],
   "source": [
    "raw_events =[]\n",
    "driver = start_selenium()\n",
    "for target_venue in venues:\n",
    "    scraping_config = target_venue.get('scraping_config', {})\n",
    "    pagination = scraping_config.get('pagination', {})\n",
    "    selectors = scraping_config.get('selectors', {})\n",
    "    id = target_venue.get('venue_id')\n",
    "    print(target_venue.get('name'))\n",
    "    events_list=[]\n",
    "\n",
    "    soups = []\n",
    "\n",
    "    if pagination.get('enabled'):\n",
    "        url_pattern = pagination.get('url_pattern')\n",
    "        if pagination.get('enabled'): # bool\n",
    "            pages = pagination.get('pages', 1)\n",
    "            url_pattern = pagination.get('url_pattern')\n",
    "            for i in range(1, 6):\n",
    "                page_url = url_pattern.format(page=i)\n",
    "                print(f\"  ‚Üí Getting page {i}: {page_url}\")\n",
    "                try:\n",
    "                    driver.get(page_url)\n",
    "                    time.sleep(1)\n",
    "                    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "                    events = scrape_venue_html(soup, 9, target_venue['scraping_config'])\n",
    "                    raw_events.extend(events)\n",
    "                    print('added ', len(events))\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"  ‚ö†Ô∏è  Timeout/error loading page {i}: {e}\")\n",
    "                    break\n",
    "\n",
    "    else:\n",
    "        driver.get(scraping_config.get('base_url'))\n",
    "        time.sleep(1)\n",
    "        method = scraping_config.get('scraping_method', 'html')\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        if method == 'html':\n",
    "            events = scrape_venue_html(soup, id, scraping_config)\n",
    "            raw_events.extend(events)\n",
    "        elif method == 'json-ld':\n",
    "            events = scrape_venue_json_ld(soup, id, scraping_config)\n",
    "            raw_events.extend(events)\n",
    "    print('added ', len(events))\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "raw_df = pd.DataFrame(raw_events)\n",
    "raw_df.to_csv('raw_events.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old functions\n",
    "def scrape_venue(venue_id, venue_name, url, scraping_config, driver):\n",
    "    \"\"\"\n",
    "    Generic scraper that works for any venue based on its config\n",
    "    Returns list of raw events: [(artist, date_text, genre, is_cancelled), ...]\n",
    "    \"\"\"\n",
    "    print(f\"üîç Scraping {venue_name}...\")\n",
    "    \n",
    "    soups = []\n",
    "    pagination = scraping_config.get('pagination', {})\n",
    "    \n",
    "    try:\n",
    "        # Handle pagination\n",
    "        if pagination.get('enabled'): # bool\n",
    "            pages = pagination.get('pages', 1)\n",
    "            url_pattern = pagination.get('url_pattern')\n",
    "            \n",
    "            for i in range(1, pages + 1):\n",
    "                page_url = url_pattern.format(page=i)\n",
    "                print(f\"  ‚Üí Getting page {i}: {page_url}\")\n",
    "                try:\n",
    "                    driver.get(page_url)\n",
    "                    time.sleep(3)\n",
    "                    soups.append(BeautifulSoup(driver.page_source, 'html.parser'))\n",
    "                except Exception as e:\n",
    "                    print(f\"  ‚ö†Ô∏è  Timeout/error loading page {i}: {e}\")\n",
    "                    continue\n",
    "        else:\n",
    "            # No pagination, just get the base URL\n",
    "            print(f\"  ‚Üí Getting {url}\")\n",
    "            driver.get(url)\n",
    "            time.sleep(3)\n",
    "            soups.append(BeautifulSoup(driver.page_source, 'html.parser'))\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Failed to scrape {venue_name}: {e}\")\n",
    "        return []\n",
    "    \n",
    "    # Extract events from all pages\n",
    "    events = []\n",
    "    selectors = scraping_config['selectors']\n",
    "    \n",
    "    for soup in soups:\n",
    "        # Find all event containers\n",
    "        event_containers = soup.select(selectors['event_container'])\n",
    "        print(f\"  ‚Üí Found {len(event_containers)} events on this page\")\n",
    "        \n",
    "        for container in event_containers:\n",
    "            try:\n",
    "                # Extract artist\n",
    "                artist_elem = container.select_one(selectors['artist'])\n",
    "                artist = artist_elem.text.strip() if artist_elem else None\n",
    "                \n",
    "                # Extract date\n",
    "                date_elem = container.select_one(selectors['date'])\n",
    "                date_text = date_elem.text.strip() if date_elem else None\n",
    "                \n",
    "                # Extract genre (if configured)\n",
    "                genre = None\n",
    "                if selectors.get('genre'):\n",
    "                    genre_elem = container.select_one(selectors['genre'])\n",
    "                    genre = genre_elem.text.strip() if genre_elem else None\n",
    "                \n",
    "                # Check if cancelled (if configured)\n",
    "                is_cancelled = False\n",
    "                if selectors.get('cancellation_indicator'):\n",
    "                    cancel_elem = container.select_one(selectors['cancellation_indicator'])\n",
    "                    if cancel_elem:\n",
    "                        cancelled_text = scraping_config.get('filters', {}).get('cancelled_text', 'Cancelled')\n",
    "                        is_cancelled = cancel_elem.text.strip() == cancelled_text\n",
    "                \n",
    "                # Only add if we got at minimum an artist and date\n",
    "                if artist and date_text:\n",
    "                    events.append({\n",
    "                        'venue_id': venue_id,\n",
    "                        'artist': artist,\n",
    "                        'date_text': date_text,\n",
    "                        'genre': genre,\n",
    "                        'is_cancelled': is_cancelled\n",
    "                    })\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ö†Ô∏è  Error parsing event: {e}\")\n",
    "                continue\n",
    "    \n",
    "    print(f\"‚úÖ Scraped {len(events)} events from {venue_name}\")\n",
    "    return events\n",
    "\n",
    "def scrape_all_venues(conn):\n",
    "    \"\"\"\n",
    "    Scrape all active venues from the database\n",
    "    Returns list of all events from all venues\n",
    "    \"\"\"\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    # Get all active venues\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT venue_id, name, url, scraping_config \n",
    "        FROM venues \n",
    "        WHERE is_active = TRUE\n",
    "        ORDER BY name;\n",
    "    \"\"\")\n",
    "    \n",
    "    venues = cur.fetchall()\n",
    "    print(f\"üìç Found {len(venues)} active venues to scrape\\n\")\n",
    "    \n",
    "    # Start selenium once for all venues\n",
    "    driver = start_selenium()\n",
    "    \n",
    "    all_events = []\n",
    "    \n",
    "    try:\n",
    "        for venue_id, name, url, config in venues:\n",
    "            events = scrape_venue(venue_id, name, url, config, driver)\n",
    "            all_events.extend(events)\n",
    "            print()  # Blank line between venues\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error scraping {name}: {e}\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "        print(\"üõë Browser closed\")\n",
    "    \n",
    "    print(f\"\\nüéâ Total events scraped: {len(all_events)}\")\n",
    "    return all_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Inserted/Updated: The Warfield (ID: 10)\n",
      "‚úÖ Inserted/Updated: The Great American Music Hall (ID: 11)\n",
      "‚úÖ Inserted/Updated: Neck of the Woods (ID: 12)\n",
      "‚úÖ Inserted/Updated: The Aggie Theatre (ID: 13)\n",
      "‚úÖ Inserted/Updated: Washington's (ID: 14)\n",
      "‚úÖ Inserted/Updated: The Mishawaka  (ID: 15)\n",
      "\n",
      "üéâ Successfully processed 6 venues!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import psycopg2\n",
    "import os\n",
    "# scrape venue\n",
    "# pagination\n",
    "venues = [\n",
    "    {\"venue_id\": \"3\", \n",
    "        \"name\": \"The Warfield\",\n",
    "        \"address\": \"982 Market Street\",\n",
    "        \"city\": \"San Francisco\",\n",
    "        \"scraping_config\": {\n",
    "            \"base_url\": \"https://www.thewarfieldtheatre.com/events\",\n",
    "            \"pagination\": {\n",
    "                \"enabled\": False,\n",
    "            },\n",
    "            \"selectors\": {\n",
    "                \"event_container\": \"div.entry.warfield.clearfix\",\n",
    "                \"artist\": \"h3.carousel_item_title_small\",\n",
    "                \"support\": 'h4.animated',\n",
    "                \"date\": \"span.date\",\n",
    "                \"genre\": None,\n",
    "                \"cancellation_indicator\": None\n",
    "            },\n",
    "            \"date_format\": \"%a, %b %d, %Y\",\n",
    "            }},\n",
    "\n",
    "                {\n",
    "                    \"venue_id\": \"2\", \n",
    "        \"name\": \"The Great American Music Hall\",\n",
    "        \"address\": \"859 O‚ÄôFarrell St.\",\n",
    "        \"city\": \"San Francisco\",\n",
    "        \"scraping_config\": {\n",
    "            \"base_url\": \"https://gamh.com/\",\n",
    "            \"pagination\": {\n",
    "                \"enabled\": False,\n",
    "            },\n",
    "            \"selectors\": {\n",
    "                \"event_container\": \"div.seetickets-list-event-content-container.position-relative\",\n",
    "                \"artist\": \"p.fs-12.headliners\",\n",
    "                \"support\": 'p.fs-12.supporting-talent',\n",
    "                \"date\": \"p.fs-18.bold.mt-1r.event-date\",\n",
    "                \"genre\": None,\n",
    "                \"cancellation_indicator\": None\n",
    "            },\n",
    "            \"date_format\": \"%a %b %d\",\n",
    "            }},\n",
    "\n",
    "                {\n",
    "                    \"venue_id\": \"4\", \n",
    "        \"name\": \"Neck of the Woods\",\n",
    "        \"address\": \"406 Clement St\",\n",
    "        \"city\": \"San Francisco\",\n",
    "        \"scraping_config\": {\n",
    "            \"base_url\": \"https://www.neckofthewoodssf.com/page/2/\",\n",
    "            \"selectors\": {\n",
    "                \"event_container\": \"div.tw-section\",\n",
    "                \"artist\": \"div.tw-name\",\n",
    "                \"support\": None,\n",
    "                \"date\": \"div.tw-event-datetime \",\n",
    "                \"genre\": None,\n",
    "                \"cancellation_indicator\": None\n",
    "            },\n",
    "            \"date_format\": \"%a, %b %d\",\n",
    "            }},\n",
    "\n",
    "{\"venue_id\": \"1\", \n",
    "    \"name\": \"The Aggie Theatre\",\n",
    "    \"address\": \"204 S College Avenue\",\n",
    "    \"city\": \"Fort Collins\",\n",
    "    \"scraping_config\": {\n",
    "        \"scraping_method\": \"json-ld\",  # or \"html\" for regular scraping\n",
    "        \"base_url\": \"https://www.fortcollinsmusichall.com/events/\",\n",
    "        \"json_keys\": {\n",
    "            \"artist\": \"performer\",\n",
    "            \"date\": \"startDate\",\n",
    "            \"venue\": \"location.name\",\n",
    "            \"url\": \"url\"\n",
    "        },\n",
    "        \"date_format\": \"iso\"\n",
    "    }\n",
    "}, \n",
    "{\"venue_id\": \"5\", \n",
    "    \"name\": \"Washington's\",\n",
    "    \"address\": \"132 Laporte Ave\",\n",
    "    \"city\": \"Fort Collins\",\n",
    "    \"scraping_config\": {\n",
    "        \"scraping_method\": \"html\",\n",
    "        \"base_url\": \"https://bohemianlivemusic.org/our-venues/washingtons/\",\n",
    "        \"selectors\": {\n",
    "            \"event_container\": \"div.elementor-element.elementor-element-18ac28d.e-flex.e-con-boxed.e-con.e-child\",\n",
    "            \"artist\": \"div.elementor-widget-container\",\n",
    "            \"support\": None,\n",
    "            \"date\": \"span.elementor-icon-list-text.elementor-post-info__item.elementor-post-info__item--type-custom\",\n",
    "            \"genre\": None,\n",
    "            \"cancellation_indicator\": None\n",
    "        },\n",
    "        \"date_format\": \"%A, %B %d @ %I:%M %p\"\n",
    "    }\n",
    "},\n",
    "{\"venue_id\": \"6\", \n",
    "    \"name\": \"The Mishawaka \",\n",
    "    \"address\": \"13714 Poudre Canyon Highway\",\n",
    "    \"city\": \"Fort Collins\",\n",
    "    \"scraping_config\": {\n",
    "        \"scraping_method\": \"html\",\n",
    "        \"base_url\": \"https://www.themishawaka.com/events/?view=list\",\n",
    "        \"selectors\": {\n",
    "            \"event_container\": \"div.col-12.eventWrapper.rhpSingleEvent.py-4.px-0\",\n",
    "            \"artist\": \"#eventTitle h2\",\n",
    "            \"support\": None,\n",
    "            \"date\": \"#eventDate\",\n",
    "            \"genre\": None,\n",
    "            \"cancellation_indicator\": None\n",
    "        },\n",
    "        \"date_format\": \"%a, %b %d, %Y\"\n",
    "    }\n",
    "}\n",
    "]\n",
    "# Connect to database\n",
    "conn = psycopg2.connect(os.getenv('DATABASE_URL_UNPOOLED'))\n",
    "cur = conn.cursor()\n",
    "\n",
    "insert_query = \"\"\"\n",
    "INSERT INTO venues (name, address, city, url, scraping_config)\n",
    "VALUES (%s, %s, %s, %s, %s)\n",
    "ON CONFLICT (url) DO UPDATE SET\n",
    "    name = EXCLUDED.name,\n",
    "    address = EXCLUDED.address,\n",
    "    city = EXCLUDED.city,\n",
    "    scraping_config = EXCLUDED.scraping_config,\n",
    "    updated_at = CURRENT_TIMESTAMP\n",
    "RETURNING venue_id, name;\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    for venue in venues:\n",
    "        cur.execute(insert_query, (\n",
    "            venue[\"name\"],\n",
    "            venue[\"address\"],\n",
    "            venue[\"city\"],\n",
    "            venue[\"scraping_config\"][\"base_url\"],  # Use base_url as url\n",
    "            json.dumps(venue[\"scraping_config\"])\n",
    "        ))\n",
    "        venue_id, name = cur.fetchone()\n",
    "        print(f\"‚úÖ Inserted/Updated: {name} (ID: {venue_id})\")\n",
    "    \n",
    "    conn.commit()\n",
    "    print(f\"\\nüéâ Successfully processed {len(venues)} venues!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    conn.rollback()\n",
    "    print(f\"‚ùå Error inserting venues: {e}\")\n",
    "    raise\n",
    "finally:\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "playlist_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
